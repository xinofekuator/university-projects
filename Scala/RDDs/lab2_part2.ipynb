{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"logo/spark.png\" alt=\"Hadoop Logo\" width=\"250\"/></p>\n",
    "# **Lab 1 - Part 2 - Spark**\n",
    "#### The following steps demonstrate how to develop a simple word count application in Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Creating a base RDD and pair RDDs **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by generating a base RDD by using a list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.rdd.ParallelCollectionRDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val wordsList = List(\"cat\", \"elephant\", \"rat\", \"rat\", \"cat\")\n",
    "val wordsRDD = sc.parallelize(wordsList)\n",
    "\n",
    "// Print out the type of wordsRDD\n",
    "println(wordsRDD.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a `map()` transformation to add the letter 's' to each string in the base RDD we just created. We'll define a function that returns the word with an 's' at the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "def makePlural(word: String): String = {\n",
    "    \"\"\"Adds an 's' to `word`.\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  No attempt is made to follow proper\n",
    "        pluralization rules.\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    \n",
    "    return word + \"s\"\n",
    "}\n",
    "\n",
    "println(makePlural(\"cat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass each item in the base RDD into a `map()` transformation that applies the `makePlural()` function to each element, and then call the `collect()` action to see the transformed RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "elephants\n",
      "rats\n",
      "rats\n",
      "cats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val pluralRDD = wordsRDD.map(makePlural(_))\n",
    "pluralRDD.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the same RDD using a *lambda* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "elephants\n",
      "rats\n",
      "rats\n",
      "cats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val pluralLambdaRDD = wordsRDD.map(_ + \"s\")\n",
    "pluralLambdaRDD.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `map()` and a *lambda* function to return the number of characters in each word. We'll `collect` this result directly into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "9\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val pluralLengths = pluralRDD.map(_.length()).collect()\n",
    "pluralLengths.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, we will create a pair consisting of `('<word>', 1)` for each word element in the RDD.  We can create the pair RDD using the `map()` transformation with a *lambda* function to create a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cat,1)\n",
      "(elephant,1)\n",
      "(rat,1)\n",
      "(rat,1)\n",
      "(cat,1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val wordPairs = wordsRDD.map((_, 1))\n",
    "wordPairs.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Counting with pair RDDs **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of times a particular word appears in the RDD. There are multiple ways to perform the counting, but some are much less efficient than others. A naive approach would be to `collect()` all of the elements and count them in the driver program. While this approach could work for small datasets, we want an approach that will work for any size dataset including terabyte- or petabyte-sized datasets. In addition, performing all of the work in the driver program is slower than performing it in parallel in the workers. For these reasons, we will use data parallel operations.\n",
    "\n",
    "Another approach is based on using the `groupByKey()` that groups all the elements of the RDD with the same key into a single list in one of the partitions. There are two problems with using `groupByKey()`: \n",
    "   + the operation requires a lot of data movement to move all the values into the appropriate partitions.\n",
    "   + the lists can be very large and could exhaust the available memory in a worker.\n",
    "  \n",
    "Use `groupByKey()` to generate a pair RDD of type `('<word>', iterator)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cat,CompactBuffer(1, 1))\n",
      "(rat,CompactBuffer(1, 1))\n",
      "(elephant,CompactBuffer(1))\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "// Note that groupByKey requires no parameters\n",
    "val wordsGrouped = wordPairs.groupByKey()\n",
    "wordsGrouped.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `groupByKey()` transformation creates an RDD containing 3 elements, each of which is a pair of a word and an iterator. Then, sum the iterator using a `map()` transformation. The result should be a pair RDD consisting of `(word, count)` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val wordCountsGrouped = wordsGrouped.map(_._2.sum)\n",
    "wordCountsGrouped.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better approach is to start from the pair RDD and then use the `reduceByKey()` transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cat,2)\n",
      "(rat,2)\n",
      "(elephant,1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "// Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "val wordCounts = wordPairs.reduceByKey(_ + _)\n",
    "wordCounts.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expert version of the code performs the `map()` to pair RDD, `reduceByKey()` transformation, and `collect()` in one statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cat,2)\n",
      "(rat,2)\n",
      "(elephant,1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val wordCountsCollected = wordsRDD.map((_, 1)).reduceByKey(_ + _).collect()\n",
    "wordCountsCollected.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: Finding unique words and a mean value **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the number of unique words in `wordsRDD`.  You can use other RDDs that you have already created to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val uniqueWords = wordsGrouped.count\n",
    "println(uniqueWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the mean number of words per unique word in `wordCounts`. Use a `reduce()` action to sum the counts in `wordCounts` and then divide by the number of unique words.  First `map()` the pair RDD `wordCounts`, which consists of (key, value) pairs, to an RDD of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1.6666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val totalCount = wordCounts.map(_._2).reduce(_ + _)\n",
    "val average = totalCount / uniqueWords.toFloat\n",
    "\n",
    "println(totalCount)\n",
    "println(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: Apply word count to a file **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section we will finish developing our word count application.  First, define a function for word counting.  This function should take in an RDD that is a list of words like `wordsRDD` and return a pair RDD that has all of the words and their associated counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cat,2)\n",
      "(rat,2)\n",
      "(elephant,1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def wordCount(wordListRDD: RDD[String]): RDD[(String, Int)] = {\n",
    "    wordListRDD.map((_, 1)).reduceByKey(_ + _)\n",
    "}\n",
    "\n",
    "wordCount(wordsRDD).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world files are more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n",
    "  + Words should be counted independent of their capitialization (e.g., \"Spark\" and \"spark\" should be counted as the same word).\n",
    "  + All punctuation should be removed.\n",
    "  + Any leading or trailing spaces on a line should be removed.\n",
    " \n",
    "Define the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use the Python `re` module to remove any text that is not a letter, number, or space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi you\n",
      "no underscore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// Just run this code\n",
    "import scala.util.matching\n",
    "\n",
    "def removePunctuation(text: String): String = {\n",
    "    text.replaceAll(\"\"\"\\p{Punct}|^\\s+|\\s+$\"\"\", \"\").toLowerCase\n",
    "}  \n",
    "\n",
    "println(removePunctuation(\"Hi, you!\"))\n",
    "println(removePunctuation(\" No under_score!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next part of this lab, we will use the \"Complete Works of William Shakespeare\", located at `data/story/shakespeare.txt`. To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `removePunctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, so that we only print 15 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1609\n",
      "2: \n",
      "3: the sonnets\n",
      "4: \n",
      "5: by william shakespeare\n",
      "6: \n",
      "7: \n",
      "8: \n",
      "9: 1\n",
      "10: from fairest creatures we desire increase\n",
      "11: that thereby beautys rose might never die\n",
      "12: but as the riper should by time decease\n",
      "13: his tender heir might bear his memory\n",
      "14: but thou contracted to thine own bright eyes\n",
      "15: feedst thy lights flame with selfsubstantial fuel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// Just run this code\n",
    "\n",
    "val fileName=\"data/story/shakespeare.txt\"\n",
    "\n",
    "val shakespeareRDD = sc.textFile(fileName, 8).map(removePunctuation)\n",
    "shakespeareRDD.zipWithIndex().take(15).map(x => (x._2 + 1) + \": \" + x._1).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the `wordcount()` function, we have to address two issues with the format of the RDD:\n",
    "  + Split each line by its spaces.\n",
    "  + Filter out empty lines.\n",
    " \n",
    "Apply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply `split()` function. You might think that a `map()` transformation is the way to do this, but think about what the result of the `split()` function will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zwaggerd\n",
      "zounds\n",
      "zounds\n",
      "zounds\n",
      "zounds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "927633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val shakespeareWordsRDD = shakespeareRDD.flatMap(_.split(\" \"))\n",
    "val shakespeareWordCount = shakespeareWordsRDD.count()\n",
    "\n",
    "shakespeareWordsRDD.top(5).foreach(println)\n",
    "shakespeareWordsRDD.takeOrdered(5).foreach(println)\n",
    "\n",
    "println(shakespeareWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to filter out the empty elements.  Remove all entries where the word is `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882996\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val shakeWordsRDD = shakespeareWordsRDD.filter(_ != \"\")\n",
    "val shakeWordCount = shakeWordsRDD.count()\n",
    "\n",
    "println(shakeWordCount)\n",
    "shakeWordsRDD.takeOrdered(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an RDD that is only words. Next, let's apply the `wordCount()` function to produce a list of word counts. Since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 27361\n",
      "and: 26028\n",
      "i: 20681\n",
      "to: 19150\n",
      "of: 17463\n",
      "a: 14593\n",
      "you: 13615\n",
      "my: 12481\n",
      "in: 10956\n",
      "that: 10890\n",
      "is: 9134\n",
      "not: 8497\n",
      "with: 7771\n",
      "me: 7769\n",
      "it: 7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val top15WordsAndCounts = wordCount(shakeWordsRDD).map(x =>(x._2,x._1)).top(15).map(x => (x._2, x._1))\n",
    "\n",
    "top15WordsAndCounts.map(x => x._1 + \": \" + x._2).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
